{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_cloud.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "osulwaFkqCM4"
      },
      "source": [
        "pip install konlpy # 한국어 형태소 분석기 konlpy 설치"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ylx5qaDbv-nB"
      },
      "source": [
        "pip install git+https://github.com/haven-jeon/PyKoSpacing.git # 딥러닝 띄어쓰기 패키지 설치"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUPfJul8Cipa"
      },
      "source": [
        "pip install tomotopy # 토픽 모델 패키지 설치"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDV8r0SK-b3I"
      },
      "source": [
        "!sudo apt-get install -y fonts-nanum # 한글 폰트 설정\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjU12OmWlESc"
      },
      "source": [
        "# 라이브러리 불러오기\n",
        "\n",
        "from konlpy.tag import *\n",
        "from pykospacing import spacing\n",
        "from nltk import Text\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from wordcloud import WordCloud\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re, natsort, glob\n",
        "import tomotopy as tp\n",
        "\n",
        "km = Komoran() # 코모란 클래스 이용\n",
        "plt.rc('font',family='NanumBarunGothic') # 한글 지정\n",
        "model = tp.LDAModel(k=7,alpha=0.1,eta=0.01,min_df=1) # 토픽 모델 생성, 토픽 7개"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrVR0Mbwn2OV"
      },
      "source": [
        "def han_fun(doc):\n",
        "  nouns = km.nouns(doc)\n",
        "  nouns = [noun for noun in nouns if len(noun)>=2] # 2글자 이상 필터링(남아있는 불용어 제거 목적)\n",
        "  return nouns\n",
        "\n",
        "text_list = natsort.natsorted(glob.glob('kp'+'*.txt'))\n",
        "\n",
        "corpus,token_doc = [], []\n",
        "for file in text_list:\n",
        "  file_data = open(file,'r',encoding='cp949').readlines()[0]\n",
        "  file_data = spacing(file_data) # 딥러닝을 이용한 띄어쓰기\n",
        "  file_data = re.sub(\"\\d+\",\" \",file_data) # 숫자 제거\n",
        "  file_data = re.sub(\"연구|논문\",\"\",file_data) # 본 연구, 본 논문과 같은 불용어 제거\n",
        "  file_data = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]','',file_data) # 특수문자 제거\n",
        "\n",
        "  token = han_fun(file_data)\n",
        "  model.add_doc(Text(token))\n",
        "  corpus.append(file_data)\n",
        "\n",
        "vector = TfidfVectorizer(tokenizer=han_fun) # TF-IDF Matrix 생성\n",
        "vector1 = CountVectorizer(tokenizer=han_fun) # TF-IDF Matrix 생성\n",
        "\n",
        "tdm = vector.fit_transform(corpus).toarray()\n",
        "tdm1 = vector1.fit_transform(corpus).toarray()\n",
        "\n",
        "column = vector.get_feature_names() # 컬럼명(단어) 얻음\n",
        "column1 = vector1.get_feature_names() # 컬럼명(단어) 얻음\n",
        "tdm = pd.DataFrame(tdm,index=text_list,columns=column) # Term-Document-Matrix 만들기\n",
        "tdm1 = pd.DataFrame(tdm1,index=text_list,columns=column1) # Term-Document-Matrix 만들기"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFFY0YDAWx_l"
      },
      "source": [
        "model.train(200)\n",
        "for i in range(model.k):\n",
        "  res = model.get_topic_words(i, top_n=5)\n",
        "  print('Topic #{}'.format(i), end='\\t')\n",
        "  print(', '.join(w for w, p in res))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scH5zrkb6uTm"
      },
      "source": [
        "buckets = [[] for _ in range(model.k)]\n",
        "for d in model.docs:\n",
        "  buckets[d.get_topics(top_n=1)[0][0]].append(d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-g--DX3_yVg"
      },
      "source": [
        "topic_model = pd.DataFrame(index=[0,1,2,3,4,5,6])\n",
        "\n",
        "for i in range(len(model.docs)):\n",
        "  a = pd.DataFrame(model.docs[i].get_topics(),columns=['index','doc_'+str(i+1)])\n",
        "  a = a.set_index(a.columns[0])\n",
        "  topic_model = topic_model.join(a)\n",
        "topic_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbvgAUuNHjUf"
      },
      "source": [
        "count1= tdm1.sum(axis=0).sort_values(ascending=False)[0:20]\n",
        "plt.figure(figsize=(15,7.5))\n",
        "count1.plot(kind=\"barh\",color='black')\n",
        "plt.xlabel(\"빈도\",fontsize=20,rotation=0) ;plt.ylabel(\"Words\",fontsize=20,rotation=30)\n",
        "plt.xticks(fontsize=15) ; plt.yticks(fontsize=15)\n",
        "plt.title(\"Total Words(Count)\",fontsize=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2z2ic0e7vnb"
      },
      "source": [
        "count= tdm.sum(axis=0).sort_values(ascending=False)[0:20]\n",
        "plt.figure(figsize=(15,7.5))\n",
        "count.plot(kind=\"barh\",color='black')\n",
        "plt.xlabel(\"빈도\",fontsize=20,rotation=0) ;plt.ylabel(\"Words\",fontsize=20,rotation=30)\n",
        "plt.xticks(fontsize=15) ; plt.yticks(fontsize=15)\n",
        "plt.title(\"Total Words(TF-IDF)\",fontsize=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12o-z31R3B9I"
      },
      "source": [
        "doc = tdm.loc[tdm.index[0]].sort_values(ascending=False) # 첫번째 문서만 분석 해보기\n",
        "doc1 = tdm1.loc[tdm1.index[0]].sort_values(ascending=False) # 첫번째 문서만 분석 해보기"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHGc7gGPHm97"
      },
      "source": [
        "plt.figure(figsize=(15,7.5))\n",
        "doc1[:20].plot(kind='barh') # 빈도 그래프 그리기\n",
        "plt.xlabel(\"빈도\",fontsize=20,rotation=0) ;plt.ylabel(\"Words\",fontsize=20,rotation=30,labelpad=50)\n",
        "plt.xticks(fontsize=15) ; plt.yticks(fontsize=15)\n",
        "plt.title(\"kp2004a(Count)\",fontsize=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ki6Lxcpj3K8F"
      },
      "source": [
        "plt.figure(figsize=(15,7.5))\n",
        "doc[:20].plot(kind='barh') # 빈도 그래프 그리기\n",
        "plt.xlabel(\"빈도\",fontsize=20,rotation=0) ;plt.ylabel(\"Words\",fontsize=20,rotation=30,labelpad=50)\n",
        "plt.xticks(fontsize=15) ; plt.yticks(fontsize=15)\n",
        "plt.title(\"kp2004a(TF-IDF)\",fontsize=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfhQTiE2IHDy"
      },
      "source": [
        "fontpath = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
        "wc=WordCloud(font_path=fontpath,relative_scaling=0.2,background_color='white')\n",
        "wc.generate_from_frequencies(dict(doc1[:30]))\n",
        "\n",
        "plt.figure(figsize=(20,20))\n",
        "plt.imshow(wc)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv9lFBzJLybA"
      },
      "source": [
        "train_df = pd.read_csv(\"ratings_train.txt\",\"\\t\") # 훈련 파일 불러오기\n",
        "train_df = train_df.iloc[0:500] # 양이 너무 많기 때문에 500개로\n",
        "test_df = pd.read_csv(\"ratings_test.txt\",\"\\t\") # 테스트 파일 불러오기\n",
        "test_df = test_df.iloc[0:10] # 역시 10개로rain_df = pd.read_csv(\"ratings_train.txt\",\"\\t\") # 훈련 파일 불러오기\n",
        "train_df = train_df.iloc[0:100] # 양이 너무 많기 때문에 100개로\n",
        "test_df = pd.read_csv(\"ratings_test.txt\",\"\\t\") # 테스트 파일 불러오기\n",
        "test_df = test_df.iloc[0:50] # 역시 50개로\n",
        "\n",
        "okt = Okt() # 토큰화를 시킬 라이브러리 로드\n",
        "\n",
        "def tokenize(doc): # 토큰화 함수 정의\n",
        "    return ['/'.join(t) for t in okt.pos(doc, norm=True, stem=True)]\n",
        "\n",
        "okt = Okt() # 토큰화를 시킬 라이브러리 로드\n",
        "\n",
        "def tokenize(doc): # 토큰화 함수 정의\n",
        "    return ['/'.join(t) for t in okt.pos(doc, norm=True, stem=True)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaoEIGVqPLVF"
      },
      "source": [
        "train_df.isnull().any() # null값 제거\n",
        "train_df['document'] = train_df['document'].fillna('')\n",
        "\n",
        "test_df.isnull().any()\n",
        "test_df['document'] = test_df['document'].fillna('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IKRu_UjPwuG"
      },
      "source": [
        "# 토큰화 과정 토큰화 과정\n",
        "train_docs = [(tokenize(row[1]), row[2]) for row in train_df.values]\n",
        "test_docs = [(tokenize(row[1]), row[2]) for row in test_df.values]\n",
        "train_docs = [(tokenize(row[1]), row[2]) for row in train_df.values]\n",
        "test_docs = [(tokenize(row[1]), row[2]) for row in test_df.values]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09eB2n30P2lp"
      },
      "source": [
        "FREQUENCY_COUNT = 100\n",
        "tokens = [t for d in train_docs for t in d[0]]\n",
        "text = Text(tokens, name='NMSC')\n",
        "selected_words = [f[0] for f in text.vocab().most_common(FREQUENCY_COUNT)]\n",
        "def term_frequency(doc):\n",
        "    return [doc.count(word) for word in selected_words]\n",
        "x_train = [term_frequency(d) for d,_ in train_docs]\n",
        "x_test = [term_frequency(d) for d,_ in test_docs]\n",
        "\n",
        "y_train = [c for _,c in train_docs]\n",
        "y_test = [c for _,c in test_docs]\n",
        "\n",
        "x_train = np.asarray(x_train).astype('float32')\n",
        "x_test = np.asarray(x_test).astype('float32')\n",
        "\n",
        "y_train = np.asarray(y_train).astype('float32')\n",
        "y_test = np.asarray(y_test).astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTWrLWcTRgB3"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(FREQUENCY_COUNT,)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmycdOARSEcy"
      },
      "source": [
        "#학습 프로세스 설정\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),\n",
        "    loss=tf.keras.losses.binary_crossentropy,\n",
        "    metrics=[tf.keras.metrics.binary_accuracy]\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfFm9OC-SGkB"
      },
      "source": [
        "#학습 데이터로 학습\n",
        "model.fit(x_train, y_train, epochs=20, batch_size=512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4ftcex3SHlD"
      },
      "source": [
        "results = model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnYBdb3ZS3-E"
      },
      "source": [
        "review = \"아주 재미 있어요\"\n",
        "token = tokenize(review)\n",
        "tf = term_frequency(token)\n",
        "data = np.expand_dims(np.asarray(tf).astype('float32'), axis=0)\n",
        "float(model.predict(data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSoSr1PbS8IV"
      },
      "source": [
        "def predict_review(review):\n",
        "    token = tokenize(review)\n",
        "    tfq = term_frequency(token)\n",
        "    data = np.expand_dims(np.asarray(tfq).astype('float32'), axis=0)\n",
        "    score = float(model.predict(data))\n",
        "    if(score > 0.5):\n",
        "        print(f\"{review} ==> 긍정 ({round(score*100)}%)\")\n",
        "    else:\n",
        "        print(f\"{review} ==> 부정 ({round((1-score)*100)}%)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiz6LZAWTHGy"
      },
      "source": [
        "predict_review(\"재미 정말 없어요\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjcbaOiOULGI"
      },
      "source": [
        "predict_review(review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgBOKSbDUX6a"
      },
      "source": [
        "## 감정분석을 위해 Keras, Tensorflow와 같은 딥러닝 패키지가 필요했습니다\n",
        "## 아래의 블로그를 참조하여 로직을 이해하는 쪽으로 공부하였습니다.\n",
        "\n",
        "### https://devtimes.com/nlp-korea-movie-review"
      ]
    }
  ]
}