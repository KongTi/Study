{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "osulwaFkqCM4"
   },
   "outputs": [],
   "source": [
    "pip install konlpy # 한국어 형태소 분석기 konlpy 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ylx5qaDbv-nB"
   },
   "outputs": [],
   "source": [
    "pip install git+https://github.com/haven-jeon/PyKoSpacing.git # 딥러닝 띄어쓰기 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hUPfJul8Cipa"
   },
   "outputs": [],
   "source": [
    "pip install tomotopy # 토픽 모델 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDV8r0SK-b3I"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get install -y fonts-nanum # 한글 폰트 설정\n",
    "!sudo fc-cache -fv\n",
    "!rm ~/.cache/matplotlib -rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MjU12OmWlESc"
   },
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "\n",
    "from konlpy.tag import *\n",
    "from pykospacing import spacing\n",
    "from nltk import Text\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, natsort, glob\n",
    "import tomotopy as tp\n",
    "\n",
    "km = Komoran() # 코모란 클래스 이용\n",
    "plt.rc('font',family='NanumBarunGothic') # 한글 지정\n",
    "model = tp.LDAModel(k=7,alpha=0.1,eta=0.01,min_df=1) # 토픽 모델 생성, 토픽 7개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FrVR0Mbwn2OV"
   },
   "outputs": [],
   "source": [
    "def han_fun(doc):\n",
    "  nouns = km.nouns(doc)\n",
    "  nouns = [noun for noun in nouns if len(noun)>=2] # 2글자 이상 필터링(남아있는 불용어 제거 목적)\n",
    "  return nouns\n",
    "\n",
    "text_list = natsort.natsorted(glob.glob('kp'+'*.txt'))\n",
    "\n",
    "corpus,token_doc = [], []\n",
    "for file in text_list:\n",
    "  file_data = open(file,'r',encoding='cp949').readlines()[0]\n",
    "  file_data = spacing(file_data) # 딥러닝을 이용한 띄어쓰기\n",
    "  file_data = re.sub(\"\\d+\",\" \",file_data) # 숫자 제거\n",
    "  file_data = re.sub(\"연구|논문\",\"\",file_data) # 본 연구, 본 논문과 같은 불용어 제거\n",
    "  file_data = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]','',file_data) # 특수문자 제거\n",
    "\n",
    "  token = han_fun(file_data)\n",
    "  model.add_doc(Text(token))\n",
    "  corpus.append(file_data)\n",
    "\n",
    "vector = TfidfVectorizer(tokenizer=han_fun) # TF-IDF Matrix 생성\n",
    "vector1 = CountVectorizer(tokenizer=han_fun) # TF-IDF Matrix 생성\n",
    "\n",
    "tdm = vector.fit_transform(corpus).toarray()\n",
    "tdm1 = vector1.fit_transform(corpus).toarray()\n",
    "\n",
    "column = vector.get_feature_names() # 컬럼명(단어) 얻음\n",
    "column1 = vector1.get_feature_names() # 컬럼명(단어) 얻음\n",
    "tdm = pd.DataFrame(tdm,index=text_list,columns=column) # Term-Document-Matrix 만들기\n",
    "tdm1 = pd.DataFrame(tdm1,index=text_list,columns=column1) # Term-Document-Matrix 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rFFY0YDAWx_l"
   },
   "outputs": [],
   "source": [
    "model.train(200)\n",
    "for i in range(model.k):\n",
    "  res = model.get_topic_words(i, top_n=5)\n",
    "  print('Topic #{}'.format(i), end='\\t')\n",
    "  print(', '.join(w for w, p in res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scH5zrkb6uTm"
   },
   "outputs": [],
   "source": [
    "buckets = [[] for _ in range(model.k)]\n",
    "for d in model.docs:\n",
    "  buckets[d.get_topics(top_n=1)[0][0]].append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K-g--DX3_yVg"
   },
   "outputs": [],
   "source": [
    "topic_model = pd.DataFrame(index=[0,1,2,3,4,5,6])\n",
    "\n",
    "for i in range(len(model.docs)):\n",
    "  a = pd.DataFrame(model.docs[i].get_topics(),columns=['index','doc_'+str(i+1)])\n",
    "  a = a.set_index(a.columns[0])\n",
    "  topic_model = topic_model.join(a)\n",
    "topic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GbvgAUuNHjUf"
   },
   "outputs": [],
   "source": [
    "count1= tdm1.sum(axis=0).sort_values(ascending=False)[0:20]\n",
    "plt.figure(figsize=(15,7.5))\n",
    "count1.plot(kind=\"barh\",color='black')\n",
    "plt.xlabel(\"빈도\",fontsize=20,rotation=0) ;plt.ylabel(\"Words\",fontsize=20,rotation=30)\n",
    "plt.xticks(fontsize=15) ; plt.yticks(fontsize=15)\n",
    "plt.title(\"Total Words(Count)\",fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2z2ic0e7vnb"
   },
   "outputs": [],
   "source": [
    "count= tdm.sum(axis=0).sort_values(ascending=False)[0:20]\n",
    "plt.figure(figsize=(15,7.5))\n",
    "count.plot(kind=\"barh\",color='black')\n",
    "plt.xlabel(\"빈도\",fontsize=20,rotation=0) ;plt.ylabel(\"Words\",fontsize=20,rotation=30)\n",
    "plt.xticks(fontsize=15) ; plt.yticks(fontsize=15)\n",
    "plt.title(\"Total Words(TF-IDF)\",fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12o-z31R3B9I"
   },
   "outputs": [],
   "source": [
    "doc = tdm.loc[tdm.index[0]].sort_values(ascending=False) # 첫번째 문서만 분석 해보기\n",
    "doc1 = tdm1.loc[tdm1.index[0]].sort_values(ascending=False) # 첫번째 문서만 분석 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHGc7gGPHm97"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7.5))\n",
    "doc1[:20].plot(kind='barh') # 빈도 그래프 그리기\n",
    "plt.xlabel(\"빈도\",fontsize=20,rotation=0) ;plt.ylabel(\"Words\",fontsize=20,rotation=30,labelpad=50)\n",
    "plt.xticks(fontsize=15) ; plt.yticks(fontsize=15)\n",
    "plt.title(\"kp2004a(Count)\",fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ki6Lxcpj3K8F"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7.5))\n",
    "doc[:20].plot(kind='barh') # 빈도 그래프 그리기\n",
    "plt.xlabel(\"빈도\",fontsize=20,rotation=0) ;plt.ylabel(\"Words\",fontsize=20,rotation=30,labelpad=50)\n",
    "plt.xticks(fontsize=15) ; plt.yticks(fontsize=15)\n",
    "plt.title(\"kp2004a(TF-IDF)\",fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qfhQTiE2IHDy"
   },
   "outputs": [],
   "source": [
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
    "wc=WordCloud(font_path=fontpath,relative_scaling=0.2,background_color='white')\n",
    "wc.generate_from_frequencies(dict(doc1[:30]))\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gv9lFBzJLybA"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"ratings_train.txt\",\"\\t\") # 훈련 파일 불러오기\n",
    "train_df = train_df.iloc[0:500] # 양이 너무 많기 때문에 500개로\n",
    "test_df = pd.read_csv(\"ratings_test.txt\",\"\\t\") # 테스트 파일 불러오기\n",
    "test_df = test_df.iloc[0:10] # 역시 10개로rain_df = pd.read_csv(\"ratings_train.txt\",\"\\t\") # 훈련 파일 불러오기\n",
    "train_df = train_df.iloc[0:100] # 양이 너무 많기 때문에 100개로\n",
    "test_df = pd.read_csv(\"ratings_test.txt\",\"\\t\") # 테스트 파일 불러오기\n",
    "test_df = test_df.iloc[0:50] # 역시 50개로\n",
    "\n",
    "okt = Okt() # 토큰화를 시킬 라이브러리 로드\n",
    "\n",
    "def tokenize(doc): # 토큰화 함수 정의\n",
    "    return ['/'.join(t) for t in okt.pos(doc, norm=True, stem=True)]\n",
    "\n",
    "okt = Okt() # 토큰화를 시킬 라이브러리 로드\n",
    "\n",
    "def tokenize(doc): # 토큰화 함수 정의\n",
    "    return ['/'.join(t) for t in okt.pos(doc, norm=True, stem=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GaoEIGVqPLVF"
   },
   "outputs": [],
   "source": [
    "train_df.isnull().any() # null값 제거\n",
    "train_df['document'] = train_df['document'].fillna('')\n",
    "\n",
    "test_df.isnull().any()\n",
    "test_df['document'] = test_df['document'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_IKRu_UjPwuG"
   },
   "outputs": [],
   "source": [
    "# 토큰화 과정 토큰화 과정\n",
    "train_docs = [(tokenize(row[1]), row[2]) for row in train_df.values]\n",
    "test_docs = [(tokenize(row[1]), row[2]) for row in test_df.values]\n",
    "train_docs = [(tokenize(row[1]), row[2]) for row in train_df.values]\n",
    "test_docs = [(tokenize(row[1]), row[2]) for row in test_df.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09eB2n30P2lp"
   },
   "outputs": [],
   "source": [
    "FREQUENCY_COUNT = 100\n",
    "tokens = [t for d in train_docs for t in d[0]]\n",
    "text = Text(tokens, name='NMSC')\n",
    "selected_words = [f[0] for f in text.vocab().most_common(FREQUENCY_COUNT)]\n",
    "def term_frequency(doc):\n",
    "    return [doc.count(word) for word in selected_words]\n",
    "x_train = [term_frequency(d) for d,_ in train_docs]\n",
    "x_test = [term_frequency(d) for d,_ in test_docs]\n",
    "\n",
    "y_train = [c for _,c in train_docs]\n",
    "y_test = [c for _,c in test_docs]\n",
    "\n",
    "x_train = np.asarray(x_train).astype('float32')\n",
    "x_test = np.asarray(x_test).astype('float32')\n",
    "\n",
    "y_train = np.asarray(y_train).astype('float32')\n",
    "y_test = np.asarray(y_test).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KTWrLWcTRgB3"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(FREQUENCY_COUNT,)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmycdOARSEcy"
   },
   "outputs": [],
   "source": [
    "#학습 프로세스 설정\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    metrics=[tf.keras.metrics.binary_accuracy]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KfFm9OC-SGkB"
   },
   "outputs": [],
   "source": [
    "#학습 데이터로 학습\n",
    "model.fit(x_train, y_train, epochs=20, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4ftcex3SHlD"
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SnYBdb3ZS3-E"
   },
   "outputs": [],
   "source": [
    "review = \"아주 재미 있어요\"\n",
    "token = tokenize(review)\n",
    "tf = term_frequency(token)\n",
    "data = np.expand_dims(np.asarray(tf).astype('float32'), axis=0)\n",
    "float(model.predict(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSoSr1PbS8IV"
   },
   "outputs": [],
   "source": [
    "def predict_review(review):\n",
    "    token = tokenize(review)\n",
    "    tfq = term_frequency(token)\n",
    "    data = np.expand_dims(np.asarray(tfq).astype('float32'), axis=0)\n",
    "    score = float(model.predict(data))\n",
    "    if(score > 0.5):\n",
    "        print(f\"{review} ==> 긍정 ({round(score*100)}%)\")\n",
    "    else:\n",
    "        print(f\"{review} ==> 부정 ({round((1-score)*100)}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yiz6LZAWTHGy"
   },
   "outputs": [],
   "source": [
    "predict_review(\"재미 정말 없어요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjcbaOiOULGI"
   },
   "outputs": [],
   "source": [
    "predict_review(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgBOKSbDUX6a"
   },
   "source": [
    "## 감정분석을 위해 Keras, Tensorflow와 같은 딥러닝 패키지가 필요했습니다\n",
    "## 아래의 블로그를 참조하여 로직을 이해하는 쪽으로 공부하였습니다.\n",
    "\n",
    "### https://devtimes.com/nlp-korea-movie-review"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
